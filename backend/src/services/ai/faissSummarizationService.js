/**
 * FAISS Summarization Service
 * ---------------------------
 * Handles hybrid summarization (FAISS + LLM) and PDF generation.
 */

import { searchFaissIndexFromMessages } from "../embedding/embedService.js";
import { buildSummaryPrompt } from "./summarizationService.js";
import ollama from "ollama";
import fs from "fs/promises";
import path from "path";
import handlebars from "handlebars";
import puppeteer from "puppeteer";

/* -------------------------------------------------------------------------- */
/* üîπ Markdown ‚Üí HTML Formatter (shared utility)                              */
/* -------------------------------------------------------------------------- */
export function formatSummary(summaryText) {
  const lines = summaryText.split("\n");
  let html = "";
  let inMainList = false;
  let inSubList = false;

  for (let line of lines) {
    line = line.trim();
    if (!line) continue;

    if (/^\*\*(.*?)\*\*$/.test(line)) {
      if (inSubList) { html += "</ul>"; inSubList = false; }
      if (inMainList) { html += "</ul>"; inMainList = false; }
      html += `<p><strong>${line.replace(/\*\*/g, "")}</strong></p>`;
    } else if (line.startsWith("*")) {
      if (!inMainList) { html += "<ul class='main-list'>"; inMainList = true; }
      if (inSubList) { html += "</ul>"; inSubList = false; }
      html += `<li>‚û§ ${line.replace(/^\*\s*/, "")}</li>`;
    } else if (line.startsWith("+")) {
      if (!inSubList) { html += "<ul class='sub-list'>"; inSubList = true; }
      html += `<li>‚Ä¢ ${line.replace(/^\+\s*/, "")}</li>`;
    } else {
      if (inSubList) { html += "</ul>"; inSubList = false; }
      if (inMainList) { html += "</ul>"; inMainList = false; }
      html += `<p>${line}</p>`;
    }
  }

  if (inSubList) html += "</ul>";
  if (inMainList) html += "</ul>";
  return html;
}

/* -------------------------------------------------------------------------- */
/* üß† Hybrid FAISS + Ollama Summarization Pipeline                            */
/* -------------------------------------------------------------------------- */
// export async function runHybridSummarizationPipeline(
//   groupId,
//   level = "basic",
//   modelName = "qwen3:8b",
//   { topK = 30, saveReport = false } = {}
// ) {
//   console.log(`üîç [Hybrid Summarization] group=${groupId} | level=${level} | model=${modelName}`);

//   // 1Ô∏è‚É£ Retrieve top chat chunks from FAISS
//   const searchResults = await searchFaissIndexFromMessages(groupId, "conversation summary", topK);
//   const results = searchResults?.results || [];
//   if (!results.length) throw new Error("No FAISS chunks found for this group.");

//   // 2Ô∏è‚É£ Merge retrieved text
//   const formattedChat = results
//     .map((r, i) => `${i + 1}. ${r.metadata?.sender || "User"}: ${r.metadata?.text || ""}`)
//     .join("\n");

//   // 3Ô∏è‚É£ Build summarization prompt
//   const summaryPrompt = buildSummaryPrompt(level, formattedChat);
//   const hybridPrompt = `
// You are given ${results.length} representative excerpts from a longer group conversation.
// Use them as context to summarize the *entire* discussion at a "${level}" detail level.

// ${summaryPrompt}
// `;

//   // 4Ô∏è‚É£ Generate summary via Ollama
//   const start = performance.now();
//   const aiSummary = await ollama.chat({
//     model: modelName,
//     messages: [{ role: "user", content: hybridPrompt }],
//   });
//   const duration = performance.now() - start;
//   const summaryText = aiSummary?.message?.content?.trim() || "‚ö†Ô∏è Failed summary.";

//   console.log(`‚úÖ [Hybrid Summarization] ${level} done in ${(duration / 1000).toFixed(2)} s`);

//   // 5Ô∏è‚É£ Generate PDF report (optional)
//   if (saveReport) {
//     const formattedSummaryHtml = formatSummary(summaryText);
//     const templatePath = path.resolve("src/templates", "summaryTemplate.hbs");
//     const htmlTemplate = await fs.readFile(templatePath, "utf8");
//     const compiled = handlebars.compile(htmlTemplate);
//     const htmlContent = compiled({ summary: formattedSummaryHtml });

//     const outputDir = path.resolve("src/outputs");
//     await fs.mkdir(outputDir, { recursive: true });
//     const fileName = `summary-hybrid-${level}-${groupId}-${Date.now()}.pdf`;
//     const pdfPath = path.join(outputDir, fileName);

//     const browser = await puppeteer.launch({ headless: true });
//     const page = await browser.newPage();
//     await page.setContent(htmlContent, { waitUntil: "networkidle0" });
//     await page.pdf({ path: pdfPath, format: "A4", printBackground: true });
//     await browser.close();

//     console.log(`üìÑ [Hybrid Summarization] PDF saved ‚Üí ${pdfPath}`);
//   }

//   return { summary: summaryText, duration };
// }

// export async function runHybridSummarizationPipeline(
//   groupId,
//   level = "intermediate",
//   modelName = "llama3.1:8b-instruct-q3_K_M",
//   { topK = 30, saveReport = false, messages = null } = {} // üëà add messages param
// ) {
//   console.log(`üîç [Hybrid Summarization] group=${groupId} | level=${level} | model=${modelName}`);

//   let formattedChat = "";

//   // 1Ô∏è‚É£ If raw messages are provided, use them directly
//   if (messages && Array.isArray(messages) && messages.length > 0) {
//     console.log(`üóÇÔ∏è Using ${messages.length} direct messages for summarization.`);
//     formattedChat = messages
//       .map((m, i) => `${i + 1}. ${m.sender || "User"}: ${m.text}`)
//       .join("\n");
//   } else {
//     // 2Ô∏è‚É£ Otherwise, fallback to FAISS retrieval
//     const searchResults = await searchFaissIndexFromMessages(groupId, "conversation summary", topK);
//     const results = searchResults?.results || [];
//     if (!results.length) throw new Error("No FAISS chunks found for this group.");

//     formattedChat = results
//       .map((r, i) => `${i + 1}. ${r.metadata?.sender || "User"}: ${r.metadata?.text || ""}`)
//       .join("\n");
//   }

//   // 3Ô∏è‚É£ Build summarization prompt
//   const summaryPrompt = buildSummaryPrompt(level, formattedChat);
//   const hybridPrompt = `
// You are given ${messages ? messages.length : topK} messages from a group conversation.
// Use them as context to summarize the *entire* discussion at a "${level}" detail level.

// ${summaryPrompt}
// `;

//   // 4Ô∏è‚É£ Generate summary via Ollama
//   const start = performance.now();
//   const aiSummary = await ollama.chat({
//     model: modelName,
//     messages: [{ role: "user", content: hybridPrompt }],
//   });
//   const duration = performance.now() - start;
//   const summaryText = aiSummary?.message?.content?.trim() || "‚ö†Ô∏è Failed summary.";

//   console.log(`‚úÖ [Hybrid Summarization] ${level} done in ${(duration / 1000).toFixed(2)} s`);

//   // 5Ô∏è‚É£ (optional) Generate PDF report
//   if (saveReport) {
//     const formattedSummaryHtml = formatSummary(summaryText);
//     const templatePath = path.resolve("src/templates", "summaryTemplate.hbs");
//     const htmlTemplate = await fs.readFile(templatePath, "utf8");
//     const compiled = handlebars.compile(htmlTemplate);
//     const htmlContent = compiled({ summary: formattedSummaryHtml });

//     const outputDir = path.resolve("src/outputs");
//     await fs.mkdir(outputDir, { recursive: true });
//     const fileName = `summary-hybrid-${level}-${groupId}-${Date.now()}.pdf`;
//     const pdfPath = path.join(outputDir, fileName);

//     const browser = await puppeteer.launch({ headless: true });
//     const page = await browser.newPage();
//     await page.setContent(htmlContent, { waitUntil: "networkidle0" });
//     await page.pdf({ path: pdfPath, format: "A4", printBackground: true });
//     await browser.close();

//     console.log(`üìÑ [Hybrid Summarization] PDF saved ‚Üí ${pdfPath}`);
//   }

//   return { summary: summaryText, duration };
// }

import { GoogleGenerativeAI } from "@google/generative-ai";
import dotenv from "dotenv";
dotenv.config();

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

export async function runHybridSummarizationPipeline(
  groupId,
  level = "intermediate",
  modelName = "llama3.1:8b-instruct-q3_K_M",
  { topK = 30, saveReport = false, messages = null } = {}
) {
  console.log(`üîç [Hybrid Summarization] group=${groupId} | level=${level} | model=${modelName}`);

  let formattedChat = "";

  // 1Ô∏è‚É£ Use direct messages or fallback to FAISS
  if (messages && Array.isArray(messages) && messages.length > 0) {
    console.log(`üóÇÔ∏è Using ${messages.length} direct messages for summarization.`);
    formattedChat = messages
      .map((m, i) => `${i + 1}. ${m.sender || "User"}: ${m.text}`)
      .join("\n");
  } else {
    const searchResults = await searchFaissIndexFromMessages(groupId, "conversation summary", topK);
    const results = searchResults?.results || [];
    if (!results.length) throw new Error("No FAISS chunks found for this group.");

    formattedChat = results
      .map((r, i) => `${i + 1}. ${r.metadata?.sender || "User"}: ${r.metadata?.text || ""}`)
      .join("\n");
  }

  // 2Ô∏è‚É£ Build summarization prompt
  const summaryPrompt = buildSummaryPrompt(level, formattedChat);
  const hybridPrompt = `
You are given ${messages ? messages.length : topK} messages from a group conversation.
Use them as context to summarize the *entire* discussion at a "${level}" detail level.

${summaryPrompt}
`;

  // 3Ô∏è‚É£ Choose model type
  let summaryText = "‚ö†Ô∏è No summary.";
  const start = performance.now();

  try {
    if (modelName.startsWith("gemini-")) {
      console.log("ü§ñ Using Gemini for summarization...");
      const model = genAI.getGenerativeModel({ model: modelName });
      const result = await model.generateContent(hybridPrompt);
      summaryText = result.response?.text() || "‚ö†Ô∏è No output from Gemini.";
    } else {
      console.log("üß† Using Ollama for summarization...");
      const aiSummary = await ollama.chat({
        model: modelName,
        messages: [{ role: "user", content: hybridPrompt }],
      });
      summaryText = aiSummary?.message?.content?.trim() || "‚ö†Ô∏è Failed summary.";
    }
  } catch (err) {
    console.error(`‚ùå Summarization failed: ${err.message}`);
  }

  const duration = performance.now() - start;
  console.log(`‚úÖ [Hybrid Summarization] ${level} done in ${(duration / 1000).toFixed(2)} s`);

  // 4Ô∏è‚É£ (optional) PDF generation
  if (saveReport) {
    const formattedSummaryHtml = formatSummary(summaryText);
    const templatePath = path.resolve("src/templates", "summaryTemplate.hbs");
    const htmlTemplate = await fs.readFile(templatePath, "utf8");
    const compiled = handlebars.compile(htmlTemplate);
    const htmlContent = compiled({ summary: formattedSummaryHtml });

    const outputDir = path.resolve("src/outputs");
    await fs.mkdir(outputDir, { recursive: true });
    const fileName = `summary-hybrid-${level}-${groupId}-${Date.now()}.pdf`;
    const pdfPath = path.join(outputDir, fileName);

    const browser = await puppeteer.launch({ headless: true });
    const page = await browser.newPage();
    await page.setContent(htmlContent, { waitUntil: "networkidle0" });
    await page.pdf({ path: pdfPath, format: "A4", printBackground: true });
    await browser.close();

    console.log(`üìÑ [Hybrid Summarization] PDF saved ‚Üí ${pdfPath}`);
  }

  return { summary: summaryText, duration };
}