/**
 * RAG Service
 * -----------
 * Handles:
 *   1. "@aria-chat/" â†’ Retrieval-Augmented Q&A using FAISS context.
 *   2. "@aria-saved" â†’ Embeds group chat messages into FAISS DB.
 */

import { searchFaissIndex, buildFaissIndexFromMessages, searchFaissIndexFromMessages } from "../embedding/embedService.js";
import { rerankChunks } from "../retrieval/rerankerHelper.js";
import { multiHopRagQuery } from "./multiHopRagService.js";
import { getEmbedding } from "../embedding/ollamaEmbeddings.js";
import { getTopChunks } from "../retrieval/retrieverService.js";
import GroupMessage from "../../models/groupMessage.model.js";
import ollama from "ollama";

/* -------------------------------------------------------------------------- */
/* ðŸ§  Step 1: Save Group Messages to FAISS (for @aria-saved) â€” SMART VERSION  */
/* -------------------------------------------------------------------------- */
export async function saveGroupMessagesToVectorDB(groupId) {
  console.log(`ðŸ’¾ [RAG] Smart embedding for group ${groupId}`);

  const allMessages = await GroupMessage.find({ groupId })
    .sort({ createdAt: 1 })
    .populate("senderId", "fullName");

  if (!allMessages.length) throw new Error("No messages found to save.");

  /* ---------------------------------------------------------------------- */
  /* ðŸ”¹ Step 1: Merge small messages into semantic blocks                   */
  /* ---------------------------------------------------------------------- */
  const MAX_GAP_MINUTES = 3;
  const mergedBlocks = [];
  let block = [];

  for (let i = 0; i < allMessages.length; i++) {
    const msg = allMessages[i];
    const prev = allMessages[i - 1];
    const timeGap = prev ? (msg.createdAt - prev.createdAt) / 60000 : Infinity;

    if (
      prev &&
      msg.senderId?._id?.equals(prev.senderId?._id) &&
      timeGap < MAX_GAP_MINUTES
    ) {
      block.push(msg);
    } else {
      if (block.length) mergedBlocks.push(block);
      block = [msg];
    }
  }
  if (block.length) mergedBlocks.push(block);

  console.log(`ðŸ§© [Context Merge] ${mergedBlocks.length} logical message segments created`);

  /* ---------------------------------------------------------------------- */
  /* ðŸ”¹ Step 2: Prepare text preprocessor                                   */
  /* ---------------------------------------------------------------------- */
  function preprocessText(text) {
    return text
      .replace(/@[\w]+/g, "") // remove mentions
      .replace(/https?:\/\/\S+/g, "[link]") // replace URLs
      .replace(/\s+/g, " ") // normalize whitespace
      .trim();
  }

  /* ---------------------------------------------------------------------- */
  /* ðŸ”¹ Step 3: Batch embedding with throttling                             */
  /* ---------------------------------------------------------------------- */
  const vectors = [];
  const metadata = [];
  const BATCH_SIZE = 5;

  for (let i = 0; i < mergedBlocks.length; i += BATCH_SIZE) {
    const batch = mergedBlocks.slice(i, i + BATCH_SIZE);

    const embedPromises = batch.map(async (segment) => {
      const sender = segment[0].senderId?.fullName || "Unknown";
      const blockStart = segment[0].createdAt;
      const combined = segment
        .map((m) => `${m.senderId.fullName}: ${m.text}`)
        .join("\n");

      const cleanText = preprocessText(combined);
      const embedding = await getEmbedding(cleanText);

      return {
        embedding,
        meta: {
          sender,
          text: cleanText,
          timestamp: blockStart,
          messageCount: segment.length,
          summary: cleanText.slice(0, 160),
        },
      };
    });

    const results = await Promise.allSettled(embedPromises);

    results.forEach((res) => {
      if (res.status === "fulfilled") {
        vectors.push(res.value.embedding);
        metadata.push(res.value.meta);
      }
    });

    // Small pause to avoid flooding the embedding model
    await new Promise((r) => setTimeout(r, 100));
  }

  console.log(`ðŸ§  [Embedding] Generated ${vectors.length} context embeddings`);

  /* ---------------------------------------------------------------------- */
  /* ðŸ”¹ Step 4: Upload to FAISS backend                                    */
  /* ---------------------------------------------------------------------- */
  const result = await buildFaissIndexFromMessages(groupId, vectors, metadata);
  console.log(`âœ… [RAG] FAISS index updated for ${groupId} (${vectors.length} entries)`);

  return vectors.length;
}

/* -------------------------------------------------------------------------- */
/* ðŸ§© Step 2: Perform Retrieval-Augmented Query (for @aria-chat/)             */
/* -------------------------------------------------------------------------- */
export async function runRagQuery(groupId, question, topK = 5, mode = "chat") {
  console.log(`ðŸ§  [RAG] Running ${mode}-RAG for group ${groupId}`);
  console.log(`â“ Query: ${question}`);

  /* ---------------------------------------------------------------------- */
  /* ðŸ” Multi-hop detection (applies to both modes)                         */
  /* ---------------------------------------------------------------------- */
  const multiHopTrigger =
    question.split(" and ").length > 1 ||
    (question.match(/\?/g) || []).length > 1 ||
    /compare|relationship|difference|both|between/i.test(question);

  if (multiHopTrigger) {
    console.log("ðŸ” Detected potential multi-hop query â€” using multiHopRagQuery()");
    const { finalAnswer } = await multiHopRagQuery(groupId, question, mode, { topK });
    return finalAnswer;
  }

  /* ---------------------------------------------------------------------- */
  /* ðŸ§© Mode: DOCUMENT RAG (existing behavior)                              */
  /* ---------------------------------------------------------------------- */
  if (mode === "doc") {
    let expandedQuery = question;
    const expansionTerms =
      "predictive maintenance, process optimization, industrial automation, factory AI, smart manufacturing";

    if (/manufacturing|factory|industrial/i.test(question)) {
      expandedQuery += `. Related to: ${expansionTerms}`;
    }

    console.log(`ðŸ§© [Query Expansion] Expanded query: ${expandedQuery}`);

    const hybridResults = await getTopChunks(groupId, expandedQuery, { topK: 8 });
    if (!hybridResults?.length) return "I donâ€™t know based on the provided content.";

    console.log("ðŸ§¾ [RAG Debug] Retrieved (doc, sample 2):",
      hybridResults.slice(0, 2).map(r => ({
        header: r.header,
        snippet: r.text.slice(0, 120)
      }))
    );

    const flatChunks = hybridResults.map(r => ({
      text: r.text,
      header: r.header || "Document",
    }));
    const reranked = await rerankChunks(question, flatChunks);

    const normalized = reranked.map((r, i) => ({
      text: r.text,
      header: r.header,
      hybridScore: (1 / (i + 1)) * 0.4 + (r.rerankScore ?? 0) * 0.6,
    }));
    normalized.sort((a, b) => b.hybridScore - a.hybridScore);

    const context = normalized
      .slice(0, topK)
      .map((r, i) => `(${i + 1}) [${r.header}]\n${r.text.slice(0, 800)}`)
      .join("\n\n");

    const ragPrompt = `
You are Aria, an AI assistant that answers based on retrieved document context.
If the information is unclear or incomplete, infer responsibly but never invent facts.

---
[CONTEXT START]
${context}
[CONTEXT END]
---

User Question: ${question}
Answer:
`;

    const response = await ollama.chat({
      model: process.env.ARIA_RAG_MODEL || "llama3.1:8b-instruct-q4_K_M",
      messages: [{ role: "user", content: ragPrompt }],
    });

    return {
      answer: response?.message?.content?.trim() || "âš ï¸ No valid answer returned.",
      retrievedChunks: normalized.slice(0, topK).map(r => `${r.header}: ${r.text.slice(0, 500)}`),
    };
  }

  /* ---------------------------------------------------------------------- */
  /* ðŸ§© Mode: CHAT RAG (new logic)                                          */
  /* ---------------------------------------------------------------------- */
  if (mode === "chat") {
    console.log("ðŸ’¬ [Chat-RAG] Retrieving from FAISS chat indexâ€¦");

    const searchResults = await searchFaissIndexFromMessages(groupId, question, topK * 2);
    const retrieved = searchResults?.results || [];

    if (!retrieved.length) {
      console.warn("âš ï¸ [Chat-RAG] No context found in chat FAISS index.");
      return "I donâ€™t know based on the recent chat history.";
    }

    console.log("ðŸ§¾ [Chat-RAG Debug] Retrieved (sample 2):",
      retrieved.slice(0, 2).map(r => ({
        sender: r.metadata?.sender,
        snippet: r.metadata?.text?.slice(0, 120)
      }))
    );

    // ðŸ”„ Rerank by relevance
    const flatChunks = retrieved.map(r => ({
      text: r.metadata?.text || "",
      header: r.metadata?.sender || "User",
    }));
    const reranked = await rerankChunks(question, flatChunks);

    // ðŸ§© Weighted score: Rerank + recency
    const normalized = reranked.map((r, i) => ({
      text: r.text,
      header: r.header,
      hybridScore:
        (1 / (i + 1)) * 0.3 +
        (r.rerankScore ?? 0) * 0.6 +
        Math.random() * 0.1, // small jitter to mix close scores
    }));
    normalized.sort((a, b) => b.hybridScore - a.hybridScore);

    console.log("ðŸ§¾ [Chat-RAG Debug â€“ Top Retrieved]");
    normalized.slice(0, topK).forEach((r, i) =>
      console.log(`#${i + 1} | ${r.header}: ${r.text.slice(0, 100)}`)
    );

    const context = normalized
      .slice(0, topK)
      .map(
        (r, i) =>
          `(${i + 1}) [${r.header}]\n${r.text.slice(0, 600)}`
      )
      .join("\n\n");

    const enrichedContext = context.replace(
      /(['"â€œâ€â€˜â€™][^'"â€œâ€â€˜â€™]{3,80}['"â€œâ€â€˜â€™])|(approved|final|confirmed|locked)/gi,
      (match) => `**${match}**`
    );

    const chatPrompt = `
You are Aria, the chat assistant. Your task is to infer accurate answers based on the conversation context.

Below is a segment of group chat messages. They may include questions, confirmations, or approvals.

---
[CHAT CONTEXT]
${enrichedContext}
[END CONTEXT]
---

When answering, follow these rules:
1. Identify if the question was answered or confirmed later in the chat.
2. If someone approved or finalized something (e.g. a tagline, date, or decision), return **that specific item**.
3. If the answer isnâ€™t explicitly stated but is clearly implied (e.g., â€œYepâ€”â€˜Smart Chat, Local Mindâ€™. Approved yesterdayâ€), treat it as confirmed.
4. Only say â€œI donâ€™t knowâ€ if **no related discussion exists at all**.

User asked: ${question}

Now answer naturally, as if youâ€™re summarizing the teamâ€™s latest consensus:
`;

    const response = await ollama.chat({
      model: process.env.ARIA_RAG_MODEL || "llama3.1:8b-instruct-q4_K_M",
      messages: [{ role: "user", content: chatPrompt }],
    });

    const answer = response?.message?.content?.trim() || "âš ï¸ No valid answer returned.";
    console.log("âœ… [Chat-RAG] Answer generated successfully.");
    // ðŸ§¾ Return both answer and retrieved context
    return {
      answer,
      retrievedChunks: normalized.slice(0, topK).map((r) => {
        const snippet = r.text.replace(/\s+/g, " ").trim();
        return `${r.header}: ${snippet}`;
      }),
    };
  }
}