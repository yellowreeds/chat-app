{"version":3,"file":"chat_models.js","names":["fields?: ChatOllamaInput","model: string","options?: PullModelOptions","tools: BindToolsInput[]","kwargs?: Partial<this[\"ParsedCallOptions\"]>","options: this[\"ParsedCallOptions\"]","options?: this[\"ParsedCallOptions\"]","m: { name: string }","messages: BaseMessage[]","runManager?: CallbackManagerForLLMRun","finalChunk: AIMessageChunk | undefined","usageMetadata: UsageMetadata","lastMetadata: Omit<OllamaChatResponse, \"message\"> | undefined","outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>","config?: StructuredOutputMethodOptions<boolean>","llm: Runnable<BaseLanguageModelInput>","outputParser: Runnable<AIMessageChunk, RunOutput>","openAIFunctionDefinition: FunctionDefinition","input: any","config"],"sources":["../src/chat_models.ts"],"sourcesContent":["import {\n  AIMessage,\n  AIMessageChunk,\n  UsageMetadata,\n  type BaseMessage,\n} from \"@langchain/core/messages\";\nimport {\n  BaseLanguageModelInput,\n  StructuredOutputMethodOptions,\n  FunctionDefinition,\n} from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  type BaseChatModelParams,\n  BaseChatModel,\n  LangSmithParams,\n  BaseChatModelCallOptions,\n  BindToolsInput,\n} from \"@langchain/core/language_models/chat_models\";\n// eslint-disable-next-line @typescript-eslint/ban-ts-comment\n// @ts-ignore CJS type resolution workaround\nimport { Ollama } from \"ollama/browser\";\nimport { ChatGenerationChunk, ChatResult } from \"@langchain/core/outputs\";\nimport type {\n  ChatRequest as OllamaChatRequest,\n  ChatResponse as OllamaChatResponse,\n  Message as OllamaMessage,\n  Tool as OllamaTool,\n} from \"ollama\";\nimport {\n  Runnable,\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"@langchain/core/runnables\";\nimport { convertToOpenAITool } from \"@langchain/core/utils/function_calling\";\nimport { concat } from \"@langchain/core/utils/stream\";\nimport {\n  JsonOutputParser,\n  StructuredOutputParser,\n} from \"@langchain/core/output_parsers\";\nimport { JsonOutputKeyToolsParser } from \"@langchain/core/output_parsers/openai_tools\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport {\n  InteropZodType,\n  isInteropZodSchema,\n} from \"@langchain/core/utils/types\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\nimport {\n  convertOllamaMessagesToLangChain,\n  convertToOllamaMessages,\n} from \"./utils.js\";\nimport { OllamaCamelCaseOptions } from \"./types.js\";\n\nexport interface ChatOllamaCallOptions extends BaseChatModelCallOptions {\n  /**\n   * An array of strings to stop on.\n   */\n  stop?: string[];\n  tools?: BindToolsInput[];\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  format?: string | Record<string, any>;\n  /** @deprecated Tool choice is not supported for ChatOllama */\n  tool_choice?: never;\n}\n\nexport interface PullModelOptions {\n  /**\n   * Whether or not to stream the download.\n   * @default true\n   */\n  stream?: boolean;\n  insecure?: boolean;\n  /**\n   * Whether or not to log the status of the download\n   * to the console.\n   * @default false\n   */\n  logProgress?: boolean;\n}\n\n/**\n * Input to chat model class.\n */\nexport interface ChatOllamaInput\n  extends BaseChatModelParams,\n    OllamaCamelCaseOptions {\n  /**\n   * The model to invoke. If the model does not exist, it\n   * will be pulled.\n   * @default \"llama3\"\n   */\n  model?: string;\n  /**\n   * The host URL of the Ollama server.\n   * Defaults to `OLLAMA_BASE_URL` if set.\n   * @default \"http://127.0.0.1:11434\"\n   */\n  baseUrl?: string;\n  /**\n   * Optional HTTP Headers to include in the request.\n   */\n  headers?: Headers | Record<string, string>;\n  /**\n   * Whether or not to check the model exists on the local machine before\n   * invoking it. If set to `true`, the model will be pulled if it does not\n   * exist.\n   * @default false\n   */\n  checkOrPullModel?: boolean;\n  streaming?: boolean;\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  format?: string | Record<string, any>;\n  /**\n   * The fetch function to use.\n   * @default fetch\n   */\n  fetch?: typeof fetch;\n  think?: boolean;\n}\n\n/**\n * Ollama chat model integration.\n *\n * Setup:\n * Install `@langchain/ollama` and the Ollama app.\n *\n * ```bash\n * npm install @langchain/ollama\n * export OLLAMA_BASE_URL=\"http://127.0.0.1:11434\" # Optional; defaults to http://127.0.0.1:11434 if not set\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/_langchain_ollama.ChatOllama.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/_langchain_ollama.ChatOllamaCallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     stop: [\"\\n\"],\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatOllama } from '@langchain/ollama';\n *\n * const llm = new ChatOllama({\n *   model: \"llama-3.1:8b\",\n *   temperature: 0,\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"content\": \"The translation of \\\"I love programming\\\" into French is:\\n\\n\\\"J'adore programmer.\\\"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"model\": \"llama3.1:8b\",\n *     \"created_at\": \"2024-08-12T22:12:23.09468Z\",\n *     \"done_reason\": \"stop\",\n *     \"done\": true,\n *     \"total_duration\": 3715571291,\n *     \"load_duration\": 35244375,\n *     \"prompt_eval_count\": 19,\n *     \"prompt_eval_duration\": 3092116000,\n *     \"eval_count\": 20,\n *     \"eval_duration\": 585789000\n *   },\n *   \"tool_calls\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 19,\n *     \"output_tokens\": 20,\n *     \"total_tokens\": 39\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"The\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" translation\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" of\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" \\\"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"I\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * ...\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"model\": \"llama3.1:8b\",\n *     \"created_at\": \"2024-08-12T22:13:22.22423Z\",\n *     \"done_reason\": \"stop\",\n *     \"done\": true,\n *     \"total_duration\": 8599883208,\n *     \"load_duration\": 35975875,\n *     \"prompt_eval_count\": 19,\n *     \"prompt_eval_duration\": 7918195000,\n *     \"eval_count\": 20,\n *     \"eval_duration\": 643569000\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 19,\n *     \"output_tokens\": 20,\n *     \"total_tokens\": 39\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools([GetWeather, GetPopulation]);\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     id: '49410cad-2163-415e-bdcd-d26938a9c8c5',\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     id: '39e230e4-63ec-4fae-9df0-21c3abe735ad',\n *     type: 'tool_call'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, { name: \"Joke\" });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   punchline: 'Why did the cat join a band? Because it wanted to be the purr-cussionist!',\n *   rating: 8,\n *   setup: 'A cat walks into a music store and asks the owner...'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 19, output_tokens: 20, total_tokens: 39 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   model: 'llama3.1:8b',\n *   created_at: '2024-08-12T22:17:42.274795Z',\n *   done_reason: 'stop',\n *   done: true,\n *   total_duration: 6767071209,\n *   load_duration: 31628209,\n *   prompt_eval_count: 19,\n *   prompt_eval_duration: 6124504000,\n *   eval_count: 20,\n *   eval_duration: 608785000\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport class ChatOllama\n  extends BaseChatModel<ChatOllamaCallOptions, AIMessageChunk>\n  implements ChatOllamaInput\n{\n  // Used for tracing, replace with the same name as your class\n  static lc_name() {\n    return \"ChatOllama\";\n  }\n\n  model = \"llama3\";\n\n  numa?: boolean;\n\n  numCtx?: number;\n\n  numBatch?: number;\n\n  numGpu?: number;\n\n  mainGpu?: number;\n\n  lowVram?: boolean;\n\n  f16Kv?: boolean;\n\n  logitsAll?: boolean;\n\n  vocabOnly?: boolean;\n\n  useMmap?: boolean;\n\n  useMlock?: boolean;\n\n  embeddingOnly?: boolean;\n\n  numThread?: number;\n\n  numKeep?: number;\n\n  seed?: number;\n\n  numPredict?: number;\n\n  topK?: number;\n\n  topP?: number;\n\n  tfsZ?: number;\n\n  typicalP?: number;\n\n  repeatLastN?: number;\n\n  temperature?: number;\n\n  repeatPenalty?: number;\n\n  presencePenalty?: number;\n\n  frequencyPenalty?: number;\n\n  mirostat?: number;\n\n  mirostatTau?: number;\n\n  mirostatEta?: number;\n\n  penalizeNewline?: boolean;\n\n  streaming?: boolean;\n\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  format?: string | Record<string, any>;\n\n  keepAlive?: string | number;\n\n  client: Ollama;\n\n  checkOrPullModel = false;\n\n  baseUrl = \"http://127.0.0.1:11434\";\n\n  think?: boolean;\n\n  constructor(fields?: ChatOllamaInput) {\n    super(fields ?? {});\n\n    this.baseUrl =\n      fields?.baseUrl ??\n      getEnvironmentVariable(\"OLLAMA_BASE_URL\") ??\n      this.baseUrl;\n\n    this.client = new Ollama({\n      fetch: fields?.fetch,\n      host: this.baseUrl,\n      headers: fields?.headers,\n    });\n\n    this.model = fields?.model ?? this.model;\n    this.numa = fields?.numa;\n    this.numCtx = fields?.numCtx;\n    this.numBatch = fields?.numBatch;\n    this.numGpu = fields?.numGpu;\n    this.mainGpu = fields?.mainGpu;\n    this.lowVram = fields?.lowVram;\n    this.f16Kv = fields?.f16Kv;\n    this.logitsAll = fields?.logitsAll;\n    this.vocabOnly = fields?.vocabOnly;\n    this.useMmap = fields?.useMmap;\n    this.useMlock = fields?.useMlock;\n    this.embeddingOnly = fields?.embeddingOnly;\n    this.numThread = fields?.numThread;\n    this.numKeep = fields?.numKeep;\n    this.seed = fields?.seed;\n    this.numPredict = fields?.numPredict;\n    this.topK = fields?.topK;\n    this.topP = fields?.topP;\n    this.tfsZ = fields?.tfsZ;\n    this.typicalP = fields?.typicalP;\n    this.repeatLastN = fields?.repeatLastN;\n    this.temperature = fields?.temperature;\n    this.repeatPenalty = fields?.repeatPenalty;\n    this.presencePenalty = fields?.presencePenalty;\n    this.frequencyPenalty = fields?.frequencyPenalty;\n    this.mirostat = fields?.mirostat;\n    this.mirostatTau = fields?.mirostatTau;\n    this.mirostatEta = fields?.mirostatEta;\n    this.penalizeNewline = fields?.penalizeNewline;\n    this.streaming = fields?.streaming;\n    this.format = fields?.format;\n    this.keepAlive = fields?.keepAlive;\n    this.think = fields?.think;\n    this.checkOrPullModel = fields?.checkOrPullModel ?? this.checkOrPullModel;\n  }\n\n  // Replace\n  _llmType() {\n    return \"ollama\";\n  }\n\n  /**\n   * Download a model onto the local machine.\n   *\n   * @param {string} model The name of the model to download.\n   * @param {PullModelOptions | undefined} options Options for pulling the model.\n   * @returns {Promise<void>}\n   */\n  async pull(model: string, options?: PullModelOptions): Promise<void> {\n    const { stream, insecure, logProgress } = {\n      stream: true,\n      ...options,\n    };\n\n    if (stream) {\n      for await (const chunk of await this.client.pull({\n        model,\n        insecure,\n        stream,\n      })) {\n        if (logProgress) {\n          console.log(chunk);\n        }\n      }\n    } else {\n      const response = await this.client.pull({ model, insecure });\n      if (logProgress) {\n        console.log(response);\n      }\n    }\n  }\n\n  override bindTools(\n    tools: BindToolsInput[],\n    kwargs?: Partial<this[\"ParsedCallOptions\"]>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, ChatOllamaCallOptions> {\n    return this.withConfig({\n      tools: tools.map((tool) => convertToOpenAITool(tool)),\n      ...kwargs,\n    });\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = this.invocationParams(options);\n    return {\n      ls_provider: \"ollama\",\n      ls_model_name: this.model,\n      ls_model_type: \"chat\",\n      ls_temperature: params.options?.temperature ?? undefined,\n      ls_max_tokens: params.options?.num_predict ?? undefined,\n      ls_stop: options.stop,\n    };\n  }\n\n  invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): Omit<OllamaChatRequest, \"messages\"> {\n    return {\n      model: this.model,\n      format: options?.format ?? this.format,\n      keep_alive: this.keepAlive,\n      think: this.think,\n      options: {\n        numa: this.numa,\n        num_ctx: this.numCtx,\n        num_batch: this.numBatch,\n        num_gpu: this.numGpu,\n        main_gpu: this.mainGpu,\n        low_vram: this.lowVram,\n        f16_kv: this.f16Kv,\n        logits_all: this.logitsAll,\n        vocab_only: this.vocabOnly,\n        use_mmap: this.useMmap,\n        use_mlock: this.useMlock,\n        embedding_only: this.embeddingOnly,\n        num_thread: this.numThread,\n        num_keep: this.numKeep,\n        seed: this.seed,\n        num_predict: this.numPredict,\n        top_k: this.topK,\n        top_p: this.topP,\n        tfs_z: this.tfsZ,\n        typical_p: this.typicalP,\n        repeat_last_n: this.repeatLastN,\n        temperature: this.temperature,\n        repeat_penalty: this.repeatPenalty,\n        presence_penalty: this.presencePenalty,\n        frequency_penalty: this.frequencyPenalty,\n        mirostat: this.mirostat,\n        mirostat_tau: this.mirostatTau,\n        mirostat_eta: this.mirostatEta,\n        penalize_newline: this.penalizeNewline,\n        stop: options?.stop,\n      },\n      tools: options?.tools?.length\n        ? (options.tools.map((tool) =>\n            convertToOpenAITool(tool)\n          ) as OllamaTool[])\n        : undefined,\n    };\n  }\n\n  /**\n   * Check if a model exists on the local machine.\n   *\n   * @param {string} model The name of the model to check.\n   * @returns {Promise<boolean>} Whether or not the model exists.\n   */\n  private async checkModelExistsOnMachine(model: string): Promise<boolean> {\n    const { models } = await this.client.list();\n    return !!models.find(\n      (m: { name: string }) => m.name === model || m.name === `${model}:latest`\n    );\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    if (this.checkOrPullModel) {\n      if (!(await this.checkModelExistsOnMachine(this.model))) {\n        await this.pull(this.model, {\n          logProgress: true,\n        });\n      }\n    }\n\n    let finalChunk: AIMessageChunk | undefined;\n    for await (const chunk of this._streamResponseChunks(\n      messages,\n      options,\n      runManager\n    )) {\n      if (!finalChunk) {\n        finalChunk = chunk.message as AIMessageChunk;\n      } else {\n        finalChunk = concat(finalChunk, chunk.message as AIMessageChunk);\n      }\n    }\n\n    // Convert from AIMessageChunk to AIMessage since `generate` expects AIMessage.\n    const nonChunkMessage = new AIMessage({\n      id: finalChunk?.id,\n      content: finalChunk?.content ?? \"\",\n      additional_kwargs: finalChunk?.additional_kwargs,\n      tool_calls: finalChunk?.tool_calls,\n      response_metadata: finalChunk?.response_metadata,\n      usage_metadata: finalChunk?.usage_metadata,\n    });\n    return {\n      generations: [\n        {\n          text:\n            typeof nonChunkMessage.content === \"string\"\n              ? nonChunkMessage.content\n              : \"\",\n          message: nonChunkMessage,\n        },\n      ],\n    };\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    if (this.checkOrPullModel) {\n      if (!(await this.checkModelExistsOnMachine(this.model))) {\n        await this.pull(this.model, {\n          logProgress: true,\n        });\n      }\n    }\n\n    const params = this.invocationParams(options);\n    // TODO: remove cast after SDK adds support for tool calls\n    const ollamaMessages = convertToOllamaMessages(messages) as OllamaMessage[];\n\n    const usageMetadata: UsageMetadata = {\n      input_tokens: 0,\n      output_tokens: 0,\n      total_tokens: 0,\n    };\n\n    const stream = await this.client.chat({\n      ...params,\n      messages: ollamaMessages,\n      stream: true,\n    });\n\n    let lastMetadata: Omit<OllamaChatResponse, \"message\"> | undefined;\n\n    for await (const chunk of stream) {\n      if (options.signal?.aborted) {\n        this.client.abort();\n      }\n      const { message: responseMessage, ...rest } = chunk;\n      usageMetadata.input_tokens += rest.prompt_eval_count ?? 0;\n      usageMetadata.output_tokens += rest.eval_count ?? 0;\n      usageMetadata.total_tokens =\n        usageMetadata.input_tokens + usageMetadata.output_tokens;\n      lastMetadata = rest;\n\n      // when think is enabled, try thinking first\n      const token = this.think\n        ? responseMessage.thinking ?? responseMessage.content ?? \"\"\n        : responseMessage.content ?? \"\";\n\n      yield new ChatGenerationChunk({\n        text: token,\n        message: convertOllamaMessagesToLangChain(responseMessage),\n      });\n      await runManager?.handleLLMNewToken(token);\n    }\n\n    // Yield the `response_metadata` as the final chunk.\n    yield new ChatGenerationChunk({\n      text: \"\",\n      message: new AIMessageChunk({\n        content: \"\",\n        response_metadata: lastMetadata,\n        usage_metadata: usageMetadata,\n      }),\n    });\n  }\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<\n        BaseLanguageModelInput,\n        {\n          raw: BaseMessage;\n          parsed: RunOutput;\n        }\n      >;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<\n        BaseLanguageModelInput,\n        {\n          raw: BaseMessage;\n          parsed: RunOutput;\n        }\n      > {\n    let llm: Runnable<BaseLanguageModelInput>;\n    let outputParser: Runnable<AIMessageChunk, RunOutput>;\n\n    const { schema, name, includeRaw } = {\n      ...config,\n      schema: outputSchema,\n    };\n    const method = config?.method ?? \"jsonSchema\";\n\n    if (method === \"functionCalling\") {\n      let functionName = name ?? \"extract\";\n      if (isInteropZodSchema(schema)) {\n        const jsonSchema = toJsonSchema(schema);\n        llm = this.bindTools([\n          {\n            type: \"function\",\n            function: {\n              name: functionName,\n              description: jsonSchema.description,\n              parameters: jsonSchema,\n            },\n          },\n        ]).withConfig({\n          ls_structured_output_format: {\n            kwargs: { method },\n            schema: jsonSchema,\n          },\n        } as Partial<ChatOllamaCallOptions>);\n        outputParser = new JsonOutputKeyToolsParser({\n          returnSingle: true,\n          keyName: functionName,\n          zodSchema: schema,\n        });\n      } else {\n        let openAIFunctionDefinition: FunctionDefinition;\n        if (\n          typeof schema.name === \"string\" &&\n          typeof schema.parameters === \"object\" &&\n          schema.parameters != null\n        ) {\n          openAIFunctionDefinition = schema as FunctionDefinition;\n          functionName = schema.name;\n        } else {\n          openAIFunctionDefinition = {\n            name: functionName,\n            description: schema.description ?? \"\",\n            parameters: schema,\n          };\n        }\n        llm = this.bindTools([\n          {\n            type: \"function\",\n            function: openAIFunctionDefinition,\n          },\n        ]).withConfig({\n          ls_structured_output_format: {\n            kwargs: { method },\n            schema,\n          },\n        } as Partial<ChatOllamaCallOptions>);\n        outputParser = new JsonOutputKeyToolsParser<RunOutput>({\n          returnSingle: true,\n          keyName: functionName,\n        });\n      }\n    } else if (method === \"jsonMode\") {\n      outputParser = isInteropZodSchema(schema)\n        ? StructuredOutputParser.fromZodSchema(schema)\n        : new JsonOutputParser<RunOutput>();\n      const jsonSchema = toJsonSchema(schema);\n      llm = this.withConfig({\n        format: \"json\",\n        ls_structured_output_format: {\n          kwargs: { method },\n          schema: jsonSchema,\n        },\n      } as Partial<ChatOllamaCallOptions>);\n    } else if (method === \"jsonSchema\") {\n      outputParser = isInteropZodSchema(schema)\n        ? StructuredOutputParser.fromZodSchema(schema)\n        : new JsonOutputParser<RunOutput>();\n      const jsonSchema = toJsonSchema(schema);\n      llm = this.withConfig({\n        format: jsonSchema,\n        ls_structured_output_format: {\n          kwargs: { method },\n          schema: jsonSchema,\n        },\n      } as Partial<ChatOllamaCallOptions>);\n    } else {\n      throw new TypeError(\n        `Unrecognized structured output method '${method}'. Expected one of 'functionCalling', 'jsonMode', or 'jsonSchema'`\n      );\n    }\n\n    if (!includeRaw) {\n      return llm.pipe(outputParser).withConfig({\n        runName: \"ChatOllamaStructuredOutput\",\n      }) as Runnable<BaseLanguageModelInput, RunOutput>;\n    }\n\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input: any, config) => outputParser.invoke(input.raw, config),\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null,\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone],\n    });\n    return RunnableSequence.from<\n      BaseLanguageModelInput,\n      { raw: BaseMessage; parsed: RunOutput }\n    >([\n      {\n        raw: llm,\n      },\n      parsedWithFallback,\n    ]).withConfig({\n      runName: \"StructuredOutputRunnable\",\n    });\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAiaA,IAAa,aAAb,cACU,cAEV;CAEE,OAAO,UAAU;AACf,SAAO;CACR;CAED,QAAQ;CAER;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAGA;CAEA;CAEA;CAEA,mBAAmB;CAEnB,UAAU;CAEV;CAEA,YAAYA,QAA0B;EACpC,MAAM,UAAU,CAAE,EAAC;EAEnB,KAAK,UACH,QAAQ,WACR,uBAAuB,kBAAkB,IACzC,KAAK;EAEP,KAAK,SAAS,IAAI,OAAO;GACvB,OAAO,QAAQ;GACf,MAAM,KAAK;GACX,SAAS,QAAQ;EAClB;EAED,KAAK,QAAQ,QAAQ,SAAS,KAAK;EACnC,KAAK,OAAO,QAAQ;EACpB,KAAK,SAAS,QAAQ;EACtB,KAAK,WAAW,QAAQ;EACxB,KAAK,SAAS,QAAQ;EACtB,KAAK,UAAU,QAAQ;EACvB,KAAK,UAAU,QAAQ;EACvB,KAAK,QAAQ,QAAQ;EACrB,KAAK,YAAY,QAAQ;EACzB,KAAK,YAAY,QAAQ;EACzB,KAAK,UAAU,QAAQ;EACvB,KAAK,WAAW,QAAQ;EACxB,KAAK,gBAAgB,QAAQ;EAC7B,KAAK,YAAY,QAAQ;EACzB,KAAK,UAAU,QAAQ;EACvB,KAAK,OAAO,QAAQ;EACpB,KAAK,aAAa,QAAQ;EAC1B,KAAK,OAAO,QAAQ;EACpB,KAAK,OAAO,QAAQ;EACpB,KAAK,OAAO,QAAQ;EACpB,KAAK,WAAW,QAAQ;EACxB,KAAK,cAAc,QAAQ;EAC3B,KAAK,cAAc,QAAQ;EAC3B,KAAK,gBAAgB,QAAQ;EAC7B,KAAK,kBAAkB,QAAQ;EAC/B,KAAK,mBAAmB,QAAQ;EAChC,KAAK,WAAW,QAAQ;EACxB,KAAK,cAAc,QAAQ;EAC3B,KAAK,cAAc,QAAQ;EAC3B,KAAK,kBAAkB,QAAQ;EAC/B,KAAK,YAAY,QAAQ;EACzB,KAAK,SAAS,QAAQ;EACtB,KAAK,YAAY,QAAQ;EACzB,KAAK,QAAQ,QAAQ;EACrB,KAAK,mBAAmB,QAAQ,oBAAoB,KAAK;CAC1D;CAGD,WAAW;AACT,SAAO;CACR;;;;;;;;CASD,MAAM,KAAKC,OAAeC,SAA2C;EACnE,MAAM,EAAE,QAAQ,UAAU,aAAa,GAAG;GACxC,QAAQ;GACR,GAAG;EACJ;AAED,MAAI,QACF;cAAW,MAAM,SAAS,MAAM,KAAK,OAAO,KAAK;IAC/C;IACA;IACA;GACD,EAAC,CACA,KAAI,aACF,QAAQ,IAAI,MAAM;EAErB,OACI;GACL,MAAM,WAAW,MAAM,KAAK,OAAO,KAAK;IAAE;IAAO;GAAU,EAAC;AAC5D,OAAI,aACF,QAAQ,IAAI,SAAS;EAExB;CACF;CAED,AAAS,UACPC,OACAC,QACyE;AACzE,SAAO,KAAK,WAAW;GACrB,OAAO,MAAM,IAAI,CAAC,SAAS,oBAAoB,KAAK,CAAC;GACrD,GAAG;EACJ,EAAC;CACH;CAED,YAAYC,SAAqD;EAC/D,MAAM,SAAS,KAAK,iBAAiB,QAAQ;AAC7C,SAAO;GACL,aAAa;GACb,eAAe,KAAK;GACpB,eAAe;GACf,gBAAgB,OAAO,SAAS,eAAe;GAC/C,eAAe,OAAO,SAAS,eAAe;GAC9C,SAAS,QAAQ;EAClB;CACF;CAED,iBACEC,SACqC;AACrC,SAAO;GACL,OAAO,KAAK;GACZ,QAAQ,SAAS,UAAU,KAAK;GAChC,YAAY,KAAK;GACjB,OAAO,KAAK;GACZ,SAAS;IACP,MAAM,KAAK;IACX,SAAS,KAAK;IACd,WAAW,KAAK;IAChB,SAAS,KAAK;IACd,UAAU,KAAK;IACf,UAAU,KAAK;IACf,QAAQ,KAAK;IACb,YAAY,KAAK;IACjB,YAAY,KAAK;IACjB,UAAU,KAAK;IACf,WAAW,KAAK;IAChB,gBAAgB,KAAK;IACrB,YAAY,KAAK;IACjB,UAAU,KAAK;IACf,MAAM,KAAK;IACX,aAAa,KAAK;IAClB,OAAO,KAAK;IACZ,OAAO,KAAK;IACZ,OAAO,KAAK;IACZ,WAAW,KAAK;IAChB,eAAe,KAAK;IACpB,aAAa,KAAK;IAClB,gBAAgB,KAAK;IACrB,kBAAkB,KAAK;IACvB,mBAAmB,KAAK;IACxB,UAAU,KAAK;IACf,cAAc,KAAK;IACnB,cAAc,KAAK;IACnB,kBAAkB,KAAK;IACvB,MAAM,SAAS;GAChB;GACD,OAAO,SAAS,OAAO,SAClB,QAAQ,MAAM,IAAI,CAAC,SAClB,oBAAoB,KAAK,CAC1B,GACD;EACL;CACF;;;;;;;CAQD,MAAc,0BAA0BL,OAAiC;EACvE,MAAM,EAAE,QAAQ,GAAG,MAAM,KAAK,OAAO,MAAM;AAC3C,SAAO,CAAC,CAAC,OAAO,KACd,CAACM,MAAwB,EAAE,SAAS,SAAS,EAAE,SAAS,GAAG,MAAM,OAAO,CAAC,CAC1E;CACF;CAED,MAAM,UACJC,UACAH,SACAI,YACqB;AACrB,MAAI,KAAK,kBACP;OAAI,CAAE,MAAM,KAAK,0BAA0B,KAAK,MAAM,EACpD,MAAM,KAAK,KAAK,KAAK,OAAO,EAC1B,aAAa,KACd,EAAC;EACH;EAGH,IAAIC;AACJ,aAAW,MAAM,SAAS,KAAK,sBAC7B,UACA,SACA,WACD,CACC,KAAI,CAAC,YACH,aAAa,MAAM;OAEnB,aAAa,OAAO,YAAY,MAAM,QAA0B;EAKpE,MAAM,kBAAkB,IAAI,UAAU;GACpC,IAAI,YAAY;GAChB,SAAS,YAAY,WAAW;GAChC,mBAAmB,YAAY;GAC/B,YAAY,YAAY;GACxB,mBAAmB,YAAY;GAC/B,gBAAgB,YAAY;EAC7B;AACD,SAAO,EACL,aAAa,CACX;GACE,MACE,OAAO,gBAAgB,YAAY,WAC/B,gBAAgB,UAChB;GACN,SAAS;EACV,CACF,EACF;CACF;CAED,OAAO,sBACLF,UACAH,SACAI,YACqC;AACrC,MAAI,KAAK,kBACP;OAAI,CAAE,MAAM,KAAK,0BAA0B,KAAK,MAAM,EACpD,MAAM,KAAK,KAAK,KAAK,OAAO,EAC1B,aAAa,KACd,EAAC;EACH;EAGH,MAAM,SAAS,KAAK,iBAAiB,QAAQ;EAE7C,MAAM,iBAAiB,wBAAwB,SAAS;EAExD,MAAME,gBAA+B;GACnC,cAAc;GACd,eAAe;GACf,cAAc;EACf;EAED,MAAM,SAAS,MAAM,KAAK,OAAO,KAAK;GACpC,GAAG;GACH,UAAU;GACV,QAAQ;EACT,EAAC;EAEF,IAAIC;AAEJ,aAAW,MAAM,SAAS,QAAQ;AAChC,OAAI,QAAQ,QAAQ,SAClB,KAAK,OAAO,OAAO;GAErB,MAAM,EAAE,SAAS,gBAAiB,GAAG,MAAM,GAAG;GAC9C,cAAc,gBAAgB,KAAK,qBAAqB;GACxD,cAAc,iBAAiB,KAAK,cAAc;GAClD,cAAc,eACZ,cAAc,eAAe,cAAc;GAC7C,eAAe;GAGf,MAAM,QAAQ,KAAK,QACf,gBAAgB,YAAY,gBAAgB,WAAW,KACvD,gBAAgB,WAAW;GAE/B,MAAM,IAAI,oBAAoB;IAC5B,MAAM;IACN,SAAS,iCAAiC,gBAAgB;GAC3D;GACD,MAAM,YAAY,kBAAkB,MAAM;EAC3C;EAGD,MAAM,IAAI,oBAAoB;GAC5B,MAAM;GACN,SAAS,IAAI,eAAe;IAC1B,SAAS;IACT,mBAAmB;IACnB,gBAAgB;GACjB;EACF;CACF;CA2CD,qBAIEC,cAIAC,QASI;EACJ,IAAIC;EACJ,IAAIC;EAEJ,MAAM,EAAE,QAAQ,MAAM,YAAY,GAAG;GACnC,GAAG;GACH,QAAQ;EACT;EACD,MAAM,SAAS,QAAQ,UAAU;AAEjC,MAAI,WAAW,mBAAmB;GAChC,IAAI,eAAe,QAAQ;AAC3B,OAAI,mBAAmB,OAAO,EAAE;IAC9B,MAAM,aAAa,aAAa,OAAO;IACvC,MAAM,KAAK,UAAU,CACnB;KACE,MAAM;KACN,UAAU;MACR,MAAM;MACN,aAAa,WAAW;MACxB,YAAY;KACb;IACF,CACF,EAAC,CAAC,WAAW,EACZ,6BAA6B;KAC3B,QAAQ,EAAE,OAAQ;KAClB,QAAQ;IACT,EACF,EAAmC;IACpC,eAAe,IAAI,yBAAyB;KAC1C,cAAc;KACd,SAAS;KACT,WAAW;IACZ;GACF,OAAM;IACL,IAAIC;AACJ,QACE,OAAO,OAAO,SAAS,YACvB,OAAO,OAAO,eAAe,YAC7B,OAAO,cAAc,MACrB;KACA,2BAA2B;KAC3B,eAAe,OAAO;IACvB,OACC,2BAA2B;KACzB,MAAM;KACN,aAAa,OAAO,eAAe;KACnC,YAAY;IACb;IAEH,MAAM,KAAK,UAAU,CACnB;KACE,MAAM;KACN,UAAU;IACX,CACF,EAAC,CAAC,WAAW,EACZ,6BAA6B;KAC3B,QAAQ,EAAE,OAAQ;KAClB;IACD,EACF,EAAmC;IACpC,eAAe,IAAI,yBAAoC;KACrD,cAAc;KACd,SAAS;IACV;GACF;EACF,WAAU,WAAW,YAAY;GAChC,eAAe,mBAAmB,OAAO,GACrC,uBAAuB,cAAc,OAAO,GAC5C,IAAI;GACR,MAAM,aAAa,aAAa,OAAO;GACvC,MAAM,KAAK,WAAW;IACpB,QAAQ;IACR,6BAA6B;KAC3B,QAAQ,EAAE,OAAQ;KAClB,QAAQ;IACT;GACF,EAAmC;EACrC,WAAU,WAAW,cAAc;GAClC,eAAe,mBAAmB,OAAO,GACrC,uBAAuB,cAAc,OAAO,GAC5C,IAAI;GACR,MAAM,aAAa,aAAa,OAAO;GACvC,MAAM,KAAK,WAAW;IACpB,QAAQ;IACR,6BAA6B;KAC3B,QAAQ,EAAE,OAAQ;KAClB,QAAQ;IACT;GACF,EAAmC;EACrC,MACC,OAAM,IAAI,UACR,CAAC,uCAAuC,EAAE,OAAO,iEAAiE,CAAC;AAIvH,MAAI,CAAC,WACH,QAAO,IAAI,KAAK,aAAa,CAAC,WAAW,EACvC,SAAS,6BACV,EAAC;EAGJ,MAAM,eAAe,oBAAoB,OAAO,EAE9C,QAAQ,CAACC,OAAYC,aAAW,aAAa,OAAO,MAAM,KAAKA,SAAO,CACvE,EAAC;EACF,MAAM,aAAa,oBAAoB,OAAO,EAC5C,QAAQ,MAAM,KACf,EAAC;EACF,MAAM,qBAAqB,aAAa,cAAc,EACpD,WAAW,CAAC,UAAW,EACxB,EAAC;AACF,SAAO,iBAAiB,KAGtB,CACA,EACE,KAAK,IACN,GACD,kBACD,EAAC,CAAC,WAAW,EACZ,SAAS,2BACV,EAAC;CACH;AACF"}