{"version":3,"file":"chat_models.d.ts","names":["AIMessageChunk","BaseMessage","BaseLanguageModelInput","StructuredOutputMethodOptions","CallbackManagerForLLMRun","BaseChatModelParams","BaseChatModel","LangSmithParams","BaseChatModelCallOptions","BindToolsInput","Ollama","ChatGenerationChunk","ChatResult","ChatRequest","OllamaChatRequest","Runnable","InteropZodType","OllamaCamelCaseOptions","ChatOllamaCallOptions","Record","PullModelOptions","ChatOllamaInput","Headers","fetch","ChatOllama","RunOutput","Promise","Partial","Omit","AsyncGenerator"],"sources":["../src/chat_models.d.ts"],"sourcesContent":["import { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { BaseLanguageModelInput, StructuredOutputMethodOptions } from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { type BaseChatModelParams, BaseChatModel, LangSmithParams, BaseChatModelCallOptions, BindToolsInput } from \"@langchain/core/language_models/chat_models\";\nimport { Ollama } from \"ollama/browser\";\nimport { ChatGenerationChunk, ChatResult } from \"@langchain/core/outputs\";\nimport type { ChatRequest as OllamaChatRequest } from \"ollama\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { InteropZodType } from \"@langchain/core/utils/types\";\nimport { OllamaCamelCaseOptions } from \"./types.js\";\nexport interface ChatOllamaCallOptions extends BaseChatModelCallOptions {\n    /**\n     * An array of strings to stop on.\n     */\n    stop?: string[];\n    tools?: BindToolsInput[];\n    format?: string | Record<string, any>;\n    /** @deprecated Tool choice is not supported for ChatOllama */\n    tool_choice?: never;\n}\nexport interface PullModelOptions {\n    /**\n     * Whether or not to stream the download.\n     * @default true\n     */\n    stream?: boolean;\n    insecure?: boolean;\n    /**\n     * Whether or not to log the status of the download\n     * to the console.\n     * @default false\n     */\n    logProgress?: boolean;\n}\n/**\n * Input to chat model class.\n */\nexport interface ChatOllamaInput extends BaseChatModelParams, OllamaCamelCaseOptions {\n    /**\n     * The model to invoke. If the model does not exist, it\n     * will be pulled.\n     * @default \"llama3\"\n     */\n    model?: string;\n    /**\n     * The host URL of the Ollama server.\n     * Defaults to `OLLAMA_BASE_URL` if set.\n     * @default \"http://127.0.0.1:11434\"\n     */\n    baseUrl?: string;\n    /**\n     * Optional HTTP Headers to include in the request.\n     */\n    headers?: Headers | Record<string, string>;\n    /**\n     * Whether or not to check the model exists on the local machine before\n     * invoking it. If set to `true`, the model will be pulled if it does not\n     * exist.\n     * @default false\n     */\n    checkOrPullModel?: boolean;\n    streaming?: boolean;\n    format?: string | Record<string, any>;\n    /**\n     * The fetch function to use.\n     * @default fetch\n     */\n    fetch?: typeof fetch;\n    think?: boolean;\n}\n/**\n * Ollama chat model integration.\n *\n * Setup:\n * Install `@langchain/ollama` and the Ollama app.\n *\n * ```bash\n * npm install @langchain/ollama\n * export OLLAMA_BASE_URL=\"http://127.0.0.1:11434\" # Optional; defaults to http://127.0.0.1:11434 if not set\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/_langchain_ollama.ChatOllama.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/_langchain_ollama.ChatOllamaCallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     stop: [\"\\n\"],\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatOllama } from '@langchain/ollama';\n *\n * const llm = new ChatOllama({\n *   model: \"llama-3.1:8b\",\n *   temperature: 0,\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"content\": \"The translation of \\\"I love programming\\\" into French is:\\n\\n\\\"J'adore programmer.\\\"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"model\": \"llama3.1:8b\",\n *     \"created_at\": \"2024-08-12T22:12:23.09468Z\",\n *     \"done_reason\": \"stop\",\n *     \"done\": true,\n *     \"total_duration\": 3715571291,\n *     \"load_duration\": 35244375,\n *     \"prompt_eval_count\": 19,\n *     \"prompt_eval_duration\": 3092116000,\n *     \"eval_count\": 20,\n *     \"eval_duration\": 585789000\n *   },\n *   \"tool_calls\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 19,\n *     \"output_tokens\": 20,\n *     \"total_tokens\": 39\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"The\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" translation\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" of\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \" \\\"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"I\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * ...\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {},\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": []\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"additional_kwargs\": {},\n *   \"response_metadata\": {\n *     \"model\": \"llama3.1:8b\",\n *     \"created_at\": \"2024-08-12T22:13:22.22423Z\",\n *     \"done_reason\": \"stop\",\n *     \"done\": true,\n *     \"total_duration\": 8599883208,\n *     \"load_duration\": 35975875,\n *     \"prompt_eval_count\": 19,\n *     \"prompt_eval_duration\": 7918195000,\n *     \"eval_count\": 20,\n *     \"eval_duration\": 643569000\n *   },\n *   \"tool_calls\": [],\n *   \"tool_call_chunks\": [],\n *   \"invalid_tool_calls\": [],\n *   \"usage_metadata\": {\n *     \"input_tokens\": 19,\n *     \"output_tokens\": 20,\n *     \"total_tokens\": 39\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools([GetWeather, GetPopulation]);\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     id: '49410cad-2163-415e-bdcd-d26938a9c8c5',\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     id: '39e230e4-63ec-4fae-9df0-21c3abe735ad',\n *     type: 'tool_call'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, { name: \"Joke\" });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   punchline: 'Why did the cat join a band? Because it wanted to be the purr-cussionist!',\n *   rating: 8,\n *   setup: 'A cat walks into a music store and asks the owner...'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 19, output_tokens: 20, total_tokens: 39 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   model: 'llama3.1:8b',\n *   created_at: '2024-08-12T22:17:42.274795Z',\n *   done_reason: 'stop',\n *   done: true,\n *   total_duration: 6767071209,\n *   load_duration: 31628209,\n *   prompt_eval_count: 19,\n *   prompt_eval_duration: 6124504000,\n *   eval_count: 20,\n *   eval_duration: 608785000\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport declare class ChatOllama extends BaseChatModel<ChatOllamaCallOptions, AIMessageChunk> implements ChatOllamaInput {\n    static lc_name(): string;\n    model: string;\n    numa?: boolean;\n    numCtx?: number;\n    numBatch?: number;\n    numGpu?: number;\n    mainGpu?: number;\n    lowVram?: boolean;\n    f16Kv?: boolean;\n    logitsAll?: boolean;\n    vocabOnly?: boolean;\n    useMmap?: boolean;\n    useMlock?: boolean;\n    embeddingOnly?: boolean;\n    numThread?: number;\n    numKeep?: number;\n    seed?: number;\n    numPredict?: number;\n    topK?: number;\n    topP?: number;\n    tfsZ?: number;\n    typicalP?: number;\n    repeatLastN?: number;\n    temperature?: number;\n    repeatPenalty?: number;\n    presencePenalty?: number;\n    frequencyPenalty?: number;\n    mirostat?: number;\n    mirostatTau?: number;\n    mirostatEta?: number;\n    penalizeNewline?: boolean;\n    streaming?: boolean;\n    format?: string | Record<string, any>;\n    keepAlive?: string | number;\n    client: Ollama;\n    checkOrPullModel: boolean;\n    baseUrl: string;\n    think?: boolean;\n    constructor(fields?: ChatOllamaInput);\n    _llmType(): string;\n    /**\n     * Download a model onto the local machine.\n     *\n     * @param {string} model The name of the model to download.\n     * @param {PullModelOptions | undefined} options Options for pulling the model.\n     * @returns {Promise<void>}\n     */\n    pull(model: string, options?: PullModelOptions): Promise<void>;\n    bindTools(tools: BindToolsInput[], kwargs?: Partial<this[\"ParsedCallOptions\"]>): Runnable<BaseLanguageModelInput, AIMessageChunk, ChatOllamaCallOptions>;\n    getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n    invocationParams(options?: this[\"ParsedCallOptions\"]): Omit<OllamaChatRequest, \"messages\">;\n    private checkModelExistsOnMachine;\n    _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;\n    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n    withStructuredOutput<RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput> | Record<string, any>, config?: StructuredOutputMethodOptions<boolean>): Runnable<BaseLanguageModelInput, RunOutput> | Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n}\n//# sourceMappingURL=chat_models.d.ts.map"],"mappings":";;;;;;;;;;;;UAUiBkB,qBAAAA,SAA8BV;;AAA/C;;EAMsBW,IAAAA,CAAAA,EAAAA,MAAAA,EAAAA;EANyBX,KAAAA,CAAAA,EAKnCC,cALmCD,EAAAA;EAAwB,MAAA,CAAA,EAAA,MAAA,GAMjDW,MANiD,CAAA,MAAA,EAAA,GAAA,CAAA;EAUtDC;EAiBAC,WAAAA,CAAAA,EAAAA,KAAe;;AAgBRF,UAjCPC,gBAAAA,CAiCOD;EASFA;;;;EAzB8D,MAAA,CAAA,EAAA,OAAA;EA0U/DK,QAAAA,CAAAA,EAAAA,OAAUC;EAAuBP;;;;;EAgDpBE,WAAAA,CAAAA,EAAAA,OAAAA;;;;;AACoFpB,UA3XrGqB,eAAAA,SAAwBhB,mBA2X6EL,EA3XxDiB,sBA2XwDjB,CAAAA;EAAgBkB;;;;;EAI9GjB,KAAAA,CAAAA,EAAAA,MAAAA;EAAgEG;;;;;EACsDO,OAAAA,CAAAA,EAAAA,MAAAA;EAAfkB;;;EACZJ,OAAAA,CAAAA,EAjXrGH,OAiXqGG,GAjX3FN,MAiX2FM,CAAAA,MAAAA,EAAAA,MAAAA,CAAAA;EAAfT;;;;;;EACzDG,gBAAAA,CAAAA,EAAAA,OAAAA;EAAsBA,SAAAA,CAAAA,EAAAA,OAAAA;EAAkDM,MAAAA,CAAAA,EAAAA,MAAAA,GAzW7FN,MAyW6FM,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA;EAAfT;;;;EACvFf,KAAAA,CAAAA,EAAAA,OArWMsB,KAqWNtB;EACGwB,KAAAA,CAAAA,EAAAA,OAAAA;;;;;;;;;;;;;;;;;AA1DuG;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAAlGD,UAAAA,SAAmBlB,cAAcY,uBAAuBlB,2BAA2BqB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;oBAiClFF;;UAEVT;;;;uBAIaW;;;;;;;;;gCASSD,mBAAmBM;mBAChCjB,2BAA2BkB,qCAAqCZ,SAASb,wBAAwBF,gBAAgBkB;mDACjFX;yDACMqB,KAAKd;;sBAExCb,gEAAgEG,2BAA2BsB,QAAQd;kCACvFX,gEAAgEG,2BAA2ByB,eAAelB;yCACnGQ,sBAAsBA,mCAAmCH,eAAeS,aAAaN,8BAA8BhB,uCAAuCY,SAASb,wBAAwBuB;yCAC3LN,sBAAsBA,mCAAmCH,eAAeS,aAAaN,8BAA8BhB,sCAAsCY,SAASb;SAChMD;YACGwB;;yCAE2BN,sBAAsBA,mCAAmCH,eAAeS,aAAaN,8BAA8BhB,yCAAyCY,SAASb,wBAAwBuB,aAAaV,SAASb;SACjPD;YACGwB"}