{"version":3,"file":"llms.d.ts","names":["BaseLanguageModelCallOptions","CallbackManagerForLLMRun","GenerationChunk","StringWithAutocomplete","LLM","BaseLLMParams","Ollama","OllamaClient","OllamaCamelCaseOptions","OllamaCallOptions","OllamaInput","Headers","Record","fetch","AsyncGenerator","Promise"],"sources":["../src/llms.d.ts"],"sourcesContent":["import type { BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\nimport type { StringWithAutocomplete } from \"@langchain/core/utils/types\";\nimport { LLM, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { Ollama as OllamaClient } from \"ollama/browser\";\nimport { OllamaCamelCaseOptions } from \"./types.js\";\nexport interface OllamaCallOptions extends BaseLanguageModelCallOptions {\n    images?: string[];\n}\nexport interface OllamaInput extends BaseLLMParams, OllamaCamelCaseOptions {\n    /**\n     * The model to use when making requests.\n     * @default \"llama3\"\n     */\n    model?: string;\n    /**\n     * Optionally override the base URL to make request to.\n     * This should only be set if your Ollama instance is being\n     * server from a non-standard location.\n     * Defaults to `OLLAMA_BASE_URL` if set.\n     * @default \"http://localhost:11434\"\n     */\n    baseUrl?: string;\n    format?: string;\n    /**\n     * Optional HTTP Headers to include in the request.\n     */\n    headers?: Headers | Record<string, string>;\n    /**\n     * The fetch function to use.\n     * @default fetch\n     */\n    fetch?: typeof fetch;\n}\n/**\n * Class that represents the Ollama language model. It extends the base\n * LLM class and implements the OllamaInput interface.\n * @example\n * ```typescript\n * const ollama = new Ollama({\n *   baseUrl: \"http://api.example.com\",\n *   model: \"llama3\",\n * });\n *\n * // Streaming translation from English to German\n * const stream = await ollama.stream(\n *   `Translate \"I love programming\" into German.`\n * );\n *\n * const chunks = [];\n * for await (const chunk of stream) {\n *   chunks.push(chunk);\n * }\n *\n * console.log(chunks.join(\"\"));\n * ```\n */\nexport declare class Ollama extends LLM<OllamaCallOptions> implements OllamaInput {\n    static lc_name(): string;\n    lc_serializable: boolean;\n    model: string;\n    baseUrl: string;\n    keepAlive?: string | number;\n    embeddingOnly?: boolean;\n    f16KV?: boolean;\n    frequencyPenalty?: number;\n    logitsAll?: boolean;\n    lowVram?: boolean;\n    mainGpu?: number;\n    mirostat?: number;\n    mirostatEta?: number;\n    mirostatTau?: number;\n    numBatch?: number;\n    numCtx?: number;\n    numGpu?: number;\n    numKeep?: number;\n    numPredict?: number;\n    numThread?: number;\n    penalizeNewline?: boolean;\n    presencePenalty?: number;\n    repeatLastN?: number;\n    repeatPenalty?: number;\n    temperature?: number;\n    stop?: string[];\n    tfsZ?: number;\n    topK?: number;\n    topP?: number;\n    typicalP?: number;\n    useMLock?: boolean;\n    useMMap?: boolean;\n    vocabOnly?: boolean;\n    format?: StringWithAutocomplete<\"json\">;\n    client: OllamaClient;\n    constructor(fields?: OllamaInput & BaseLLMParams);\n    _llmType(): string;\n    invocationParams(options?: this[\"ParsedCallOptions\"]): {\n        model: string;\n        format: StringWithAutocomplete<\"json\"> | undefined;\n        keep_alive: string | number | undefined;\n        images: string[] | undefined;\n        options: {\n            embedding_only: boolean | undefined;\n            f16_kv: boolean | undefined;\n            frequency_penalty: number | undefined;\n            logits_all: boolean | undefined;\n            low_vram: boolean | undefined;\n            main_gpu: number | undefined;\n            mirostat: number | undefined;\n            mirostat_eta: number | undefined;\n            mirostat_tau: number | undefined;\n            num_batch: number | undefined;\n            num_ctx: number | undefined;\n            num_gpu: number | undefined;\n            num_keep: number | undefined;\n            num_predict: number | undefined;\n            num_thread: number | undefined;\n            penalize_newline: boolean | undefined;\n            presence_penalty: number | undefined;\n            repeat_last_n: number | undefined;\n            repeat_penalty: number | undefined;\n            temperature: number | undefined;\n            stop: string[] | undefined;\n            tfs_z: number | undefined;\n            top_k: number | undefined;\n            top_p: number | undefined;\n            typical_p: number | undefined;\n            use_mlock: boolean | undefined;\n            use_mmap: boolean | undefined;\n            vocab_only: boolean | undefined;\n        };\n    };\n    _streamResponseChunks(prompt: string, options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n    /** @ignore */\n    _call(prompt: string, options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<string>;\n}\n//# sourceMappingURL=llms.d.ts.map"],"mappings":";;;;;;;;;UAOiBS,iBAAAA,SAA0BT;;AAA3C;AAGiBU,UAAAA,WAAAA,SAAoBL,aAAT,EAAwBG,sBAAxB,CAAA;EAkBdG;;;;EAlBsCH,KAAAA,CAAAA,EAAAA,MAAAA;EAAsB;AAgD1E;;;;;;EAwCgBL,OAAAA,CAAAA,EAAAA,MAAAA;EAkC2EF,MAAAA,CAAAA,EAAAA,MAAAA;EAA0CC;;;EAE/Ba,OAAAA,CAAAA,EA1GxFJ,OA0GwFI,GA1G9EH,MA0G8EG,CAAAA,MAAAA,EAAAA,MAAAA,CAAAA;EA5ElEX;;AAA6C;;iBAzB9DS;;;;;;;;;;;;;;;;;;;;;;;;;cAyBEP,QAAAA,SAAeF,IAAIK,8BAA8BC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;WAkCzDP;UACDI;uBACaG,cAAcL;;;;YAIvBF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;yFAkC2EF,2BAA2Ba,eAAeZ;;yEAE1DD,2BAA2Bc"}